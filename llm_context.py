# -*- coding: utf-8 -*-
"""LLM_Context.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15RxPXuC-3UrJOyNt5YoLXPd_gKxlHr57
"""

!pip install langchain langchain-community faiss-cpu huggingface-hub sentence-transformers
!pip install -U langchain-huggingface==0.2.0

import os
from langchain_huggingface import ChatHuggingFace
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.tools.retriever import create_retriever_tool
from langchain import hub
from langchain.agents import create_react_agent, AgentExecutor
from langchain.tools import tool
from langchain_huggingface import HuggingFaceEndpoint
from langchain_community.embeddings import HuggingFaceEmbeddings


long_conversation_history = """
User: Hi, I need to plan a project kickoff meeting for Project Phoenix.
Agent: Of course. Project Phoenix is the new AI-driven customer analytics platform, right?
User: Yes, that's the one. The kickoff is scheduled for November 10th. Key stakeholders are Sarah, Tom, and Dr. Evans.
Agent: Got it. The core objective of Project Phoenix is to increase customer retention by 15% within two quarters.
"""
initial_documents = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50).create_documents([long_conversation_history])
vector_store = FAISS.from_documents(initial_documents, embeddings)
print("--- Initial vector store (memory) created ---")

@tool
def generate_and_add_report_to_memory(project_name: str) -> str:
    """Generates a technical report and adds it to the project's long-term memory.
    Use this when the user asks for a new, large document to be created."""
    print(f"\n--- TOOL CALLED: Generating large report for {project_name}... ---")
    base_string = "This is a line in the technical specification document for the project. It contains details about architecture, data flow, and security protocols. "
    large_output = base_string * 400000
    print(f"--- Report generated with ~{len(large_output.split())} words. ---")
    print("--- Updating memory with the new report... ---")
    report_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=20)
    new_docs = report_splitter.create_documents([large_output])
    vector_store.add_documents(new_docs)
    print("--- Memory updated successfully. ---")
    return "The technical report has been generated and saved to memory. The user can now ask questions about it."

retriever = vector_store.as_retriever()
retriever_tool = create_retriever_tool(
    retriever, "project_phoenix_retriever", "Searches and returns information from the project's long-term memory."
)

tools = [retriever_tool, generate_and_add_report_to_memory]
prompt = hub.pull("hwchase17/react")

llm = HuggingFaceEndpoint(
    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN,
    repo_id="mistralai/Mistral-7B-Instruct-v0.2",
    max_new_tokens=150,
)
model = ChatHuggingFace(llm=llm)
chat_model = ChatHuggingFace(llm=llm)
agent = create_react_agent(chat_model, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True, handle_parsing_errors=True)
print("--- Agent created successfully ---\n")

print("--- STEP 1: GENERATING THE REPORT ---")
response_1 = agent_executor.invoke(
    {"input": "Please generate the full technical report for Project Phoenix and add it to memory."}
)
print("\nAgent's Final Answer:", response_1['output'])

response = agent_executor.invoke(
        {"input": "Can you get me the full technical report for Project Phoenix?"}
    )
print(response)